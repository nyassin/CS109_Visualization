{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Scraping Historical Weather Data\n",
      "\n",
      "There is a staggering amount of weather data available, so to keep things sparse we're going to try keeping it to the 20 busiest airports in the US by total passenger traffic, as listed on <a href=http://en.wikipedia.org/wiki/List_of_the_busiest_airports_in_the_United_States> Wikipedia </a>. \n",
      "\n",
      "We will be obtaining data from the National Oceanic and Atmospheric Administration, an arm of the National Climatic Data Center, which releases extremely detailed weather datasets for weather stations around the country. Unfortunately, no one dataset gives us exactly what we want--some datasets give a large number of parameters for each station but only on a daily basis, while others only record one or two observations (e.g. temperature or wind speed) on an hourly basis. In addition, hourly temperature and precipitation are recorded separately. We want to mix and match a bit of each, so we will ultimately be using three datasets: one daily weatherset, one hourly temperature/wind speed dataset, and one hourly precipitation dataset (the latter two of which come from the ISD-Lite raw dataset). These will then be merged into a big pandas dataframe.\n",
      "\n",
      "We may also try using the <a href=http://www.wunderground.com/weather/api/> weather underground api </a> for testing of current weather conditions, after we fully analyze the historical data.\n",
      "<br/>\n",
      "API key for Ian's account: a9708ad5a6c3767a"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import requests\n",
      "from pattern import web\n",
      "from collections import defaultdict\n",
      "\n",
      "import datetime\n",
      "import urllib\n",
      "import os\n",
      "import gzip\n",
      "import time\n",
      "import csv\n",
      "import sys\n",
      "import re\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Finding airport codes **\n",
      "\n",
      "The NCDC/NOAA dataset is somewhat difficult to use because all stations are given uniquely identifying codes. Unfortunately, the inventory files that match the codes to the airport name are poorly designed--they don't use the three-letter airport codes, and searching for airport names often leads to multiple unrelated hits. For instance, searching BOS for Boston Logan Airport gives all kinds of hits for \"Boswell,\" \"Boston Dam,\" etc. Searching for full airport names is also difficult because the common phrase \"international airport\" is abbreviated ARPT, AP, APT, INTL ARPT, etc.\n",
      "\n",
      "We instead dug up a separate file that lists each station using the three-letter airport abbreviation preceded by K (KBOS, KJFK, etc.) Unfortunately, it matches these to US Air force (USAF) codes, an alternative identification scheme. So first, we needed to scrape this file for the USAF codes, and then we used these USAF codes to identify the NOAA codes for the airport stations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "airports = ['ATL', 'ORD', 'LAX', 'DFW', 'DEN', 'JFK', 'MIA', 'PHX', 'LAS', 'IAH', \n",
      "            'CLT', 'SFO', 'MCO', 'EWR', 'MSP', 'SEA', 'DTW', 'PHL', 'BOS', 'LGA']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this file contains the USAF codes we need with the airports as three-letter abbreviations\n",
      "codesurl = \"ftp://ftp.ncdc.noaa.gov/pub/data/inventories/ISH-HISTORY.TXT\"\n",
      "urllib.urlretrieve(codesurl,'maindata/usafcodelist.txt')\n",
      "\n",
      "# this file contains the NOAA codes, also listing the USAF codes but recording\n",
      "# the airport names in unreliable abbreviations\n",
      "codesurl2 = \"ftp://ftp.ncdc.noaa.gov/pub/data/normals/1981-2010/station-inventories/allstations.txt\"\n",
      "urllib.urlretrieve(codesurl2, 'maindata/noaacodelist.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "('noaacodelist.txt', <mimetools.Message instance at 0x10737a2d8>)"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this section extracts the USAF codes from the airports\n",
      "# open the file\n",
      "codelist = open('maindata/usafcodelist.txt')\n",
      "\n",
      "# all the weather stations associated with airports are designated with 'K' followed by the three-letter airport code\n",
      "# example: Boston Logan would be 'KBOS'\n",
      "# compile a regex that will search for any of the top 20 airports\n",
      "matchlist = ['K' + airport for airport in airports]\n",
      "matchpattern = '|'.join(matchlist)\n",
      "pat = re.compile(matchpattern)\n",
      "\n",
      "stations = defaultdict(lambda:defaultdict(list))\n",
      "# iterate through every line of the file and search for matching airports\n",
      "while True:\n",
      "    line = codelist.readline()\n",
      "    if line:\n",
      "        # the 51-55th char positions in each line contain the station code.\n",
      "        # if it matches one of the top 20 airports, save the corresponding codes\n",
      "        if pat.search(line[52:56]):\n",
      "            stationname = line[53:56]    # the 3-digit airport code\n",
      "            usafcode = line[0:6]\n",
      "            wbancode = line[7:12]\n",
      "            \n",
      "            # for whatever reason, some identical stations have multiple USAF codes\n",
      "            # most of these are '99999' codes that are not uniquely identifying, so throw those out\n",
      "            # but keep any secondary codes that are uniquely identifying\n",
      "            if usafcode != '999999':\n",
      "                stations[stationname]['usaf'].append(usafcode)\n",
      "                stations[stationname]['wban'].append(wbancode)\n",
      "            \n",
      "    # when last line of text file is reached, break\n",
      "    else:\n",
      "        break\n",
      "codelist.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# this section takes the USAF codes we extracted and turns them into NOAA codes\n",
      "codelist = open('maindata/noaacodelist.txt')\n",
      "\n",
      "# create another regex that will recognize any of the top 20 airports' usaf codes\n",
      "# the noaa station inventory file drops the last digit of the full usaf code\n",
      "matchlist = []\n",
      "for station in stations:\n",
      "    for code in stations[station]['usaf']:\n",
      "        matchlist.append(code[0:-1])\n",
      "matchpattern = '|'.join(matchlist)\n",
      "pat = re.compile(matchpattern)\n",
      "\n",
      "noaacodes = []\n",
      "while True:\n",
      "    line = codelist.readline()\n",
      "    if line:\n",
      "        # if it matches one of the top 20 airports, save the corresponding codes\n",
      "        usafcode = line[80:85]\n",
      "        noaacode = line[0:11]\n",
      "        if pat.match(usafcode):\n",
      "            noaacodes.append((usafcode, noaacode))\n",
      "    # when last line of text file is reached, break\n",
      "    else:\n",
      "        break\n",
      "codelist.close()\n",
      "\n",
      "# merge these codes with the existing dictionary\n",
      "for usaf, noaa in noaacodes:\n",
      "    for station in stations:\n",
      "        if re.search(str(usaf), str(stations[station]['usaf'])):\n",
      "            stations[station]['noaa'].append(noaa)\n",
      "# see the final list of codes\n",
      "for station in stations:\n",
      "    print stations[station]\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "defaultdict(<type 'list'>, {'usaf': ['722050'], 'wban': ['12815'], 'noaa': ['USW00012815']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['744860'], 'wban': ['94789'], 'noaa': ['USW00094789']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['725090'], 'wban': ['14739'], 'noaa': ['USW00014739']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['722430'], 'wban': ['12960'], 'noaa': ['USW00012960']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['724080'], 'wban': ['13739'], 'noaa': ['USW00013739']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['722020'], 'wban': ['12839'], 'noaa': ['USW00012839']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['722190'], 'wban': ['13874'], 'noaa': ['USW00013874']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['725030'], 'wban': ['14732'], 'noaa': ['USW00014732']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['726580'], 'wban': ['14922'], 'noaa': ['USW00014922']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['727930'], 'wban': ['24233'], 'noaa': ['USW00024233']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['724940'], 'wban': ['23234'], 'noaa': ['USW00023234']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['722780'], 'wban': ['23183'], 'noaa': ['USW00023183']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['722950'], 'wban': ['23174'], 'noaa': ['USW00023174']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['724670', '725650'], 'wban': ['03017', '03017'], 'noaa': ['USW00003017']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['723140'], 'wban': ['13881'], 'noaa': ['USW00013881']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['725020'], 'wban': ['14734'], 'noaa': ['USW00014734']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['725300'], 'wban': ['94846'], 'noaa': ['USW00094846']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['725370'], 'wban': ['94847'], 'noaa': ['USW00094847']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['722590'], 'wban': ['03927'], 'noaa': ['USW00003927']})\n",
        "\n",
        "defaultdict(<type 'list'>, {'usaf': ['723860'], 'wban': ['23169'], 'noaa': ['USW00023169']})\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Scraping historical daily weather data from NOAA\n",
      "\n",
      "We now have all the NOAA codes for the airports we want. The following cell, heavily adapted from an open source script by<a href=https://classic.scraperwiki.com/scrapers/noaa_historical_weather_data_csv_converter> Eldan Goldenberg </a>, scrapes temperature data from the NOAA Global Summary of the Day (GSOD) dataset and puts it into the csv format. The original script scrapes has a user input scheme to designate the year and station data to download, which we have eliminated to make the script fully automatic. In addition, we have modified the main \"downloadfiles\" function to make it far more modular. This allows us to pass our own parsers for other NOAA datasets to the same central command function (see \"scraping historical hourly weather data\").\n",
      "\n",
      "The GSOD dataset has nice summaries of each day of each station, and was our first attempt at scraping weather data. However, with limited hourly data, it is less useful for predicting flight delays. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This is the main control function. Each pass gets the user's input to pick a\n",
      "# station, and then loops over years to download the relevant files, \n",
      "# and calls the relevant parsing function to parse each one into standard CSV\n",
      "\n",
      "## MODIFICATION: added USAFcode, WBANcode, and stationname parameters\n",
      "# STATIONNAME simply designates a user-selected name for the station\n",
      "# PARSER designates which parsing function to use\n",
      "def downloadfiles(url, suffix, stationname, parser, savepath):\n",
      "\n",
      "    # Try to download the file, with very basic error handling\n",
      "    try:\n",
      "        rawfilename = stationname + 'raw' + suffix\n",
      "        urllib.urlretrieve(url,rawfilename)\n",
      "    except IOError as e:\n",
      "        print'DOWNLOADING ERROR: ', (e)\n",
      "        return\n",
      "        \n",
      "    # if we got the file without any errors, then run the parser\n",
      "    else:\n",
      "        # This function does the actual ETL\n",
      "        parser(rawfilename, stationname, savepath)\n",
      "        # clean up after ourselves\n",
      "        os.remove(rawfilename)\n",
      "    urllib.urlcleanup()\n",
      "    \n",
      "    # show progress\n",
      "    print(\"Successfully downloaded \" + str(year) + \" for station \" + stationname)\n",
      "    return 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This function goes through each downloaded file line by line, and translates\n",
      "#        it from NOAA's idiosyncratic format to CSV with all the fields separated\n",
      "#        out rationally.\n",
      "def parsegsod(filename, station, savepath):\n",
      "    # open the input file\n",
      "    f_in = gzip.open(str(filename))\n",
      "    reader = csv.reader(f_in, delimiter=' ', quoting=csv.QUOTE_NONE, skipinitialspace=True)\n",
      "\n",
      "    # open the output file and write the header row\n",
      "    f_out = open(savepath+station+'.csv','w')\n",
      "    writer = csv.writer(f_out, dialect=csv.excel)\n",
      "    writer.writerow([\"Station\", \"Year\", \"Month\", \"Day\", \\\n",
      "        \"MeanTemp\", \"NTempObs\", \"DewPoint\", \"NDewPointObs\", \\\n",
      "        \"SeaLevelPressure\", \"NSeaLevPressObs\", \"StationPressure\", \\\n",
      "        \"NStatPressObs\", \"Visibility\", \"NVisibilityObs\", \"MeanWindSpeed\", \\\n",
      "        \"NWindObs\", \"MaxSustWindSpeed\", \"MaxWindGust\", \"MaxTemp\",  \\\n",
      "        \"MaxTempSource\", \"MinTemp\", \"MinTempSource\", \"PrecipAmount\", \\\n",
      "        \"NPrecipReportHours\", \"PrecipFlag\", \"SnowDepth\", \"Fog\", \"Rain\", \\\n",
      "        \"Snow\", \"Hail\", \"Thunder\", \"Tornado\"])\n",
      "    \n",
      "    # Set up connections to input and output files. The CSV library also helps\n",
      "    #        with reading the input file, because we can treat it as space separated\n",
      "    #        with consecutive spaces being collapsed together\n",
      "\n",
      "    for row in reader:\n",
      "        if (row[0] != 'STN---'):\n",
      "            # If it's the header row, just skip; otherwise process\n",
      "            outrow = [station] # the station name is not in the input file\n",
      "\n",
      "            # skipping first 2 cols of the input file as they are the USAF & WBAN codes\n",
      "            #        which we're replacing with the actual station name.\n",
      "\n",
      "            # expanding col 3 into separate Y, M & D fields makes them easier to work\n",
      "            #        with in Tableau\n",
      "            outrow.append((row[2])[:4]) # first 4 digits are the year\n",
      "            outrow.append((row[2])[4:6]) # next 2 are the month\n",
      "            outrow.append((row[2])[-2:]) # final 2 are the day\n",
      "\n",
      "            # now we can use a loop to get through a bunch of field pairs that all\n",
      "            #        work the same:\n",
      "            # MeanTemp, NTempObs, DewPoint, NDewPointObs, SeaLevelPressure,\n",
      "            #        NSeaLevPressObs, StationPressure, NStatPressObs\n",
      "            for i in range(3, 11, 2):\n",
      "                # for each of these, 9999.9 means NULL, and the number of observations\n",
      "                #        follows the value\n",
      "                if (row[i+1] == \"0\") or (row[i] == \"9999.9\"):\n",
      "                    outrow.append(\"NULL\")\n",
      "                    outrow.append(0)\n",
      "                else:\n",
      "                    outrow.append(row[i])\n",
      "                    outrow.append(row[i+1])\n",
      "\n",
      "            # Now the same principle for Visibility, which uses a different NULL token\n",
      "            # Visibility, NVisibilityObs\n",
      "            if (row[12] == \"0\") or (row[11] == \"999.9\"):\n",
      "                outrow.append(\"NULL\")\n",
      "                outrow.append(0)\n",
      "            else:\n",
      "                outrow.append(row[11])\n",
      "                outrow.append(row[12])\n",
      "\n",
      "            # Now for wind data, which is 4 fields of which the second is the number\n",
      "            #        of observations from which the other 3 values were determined\n",
      "            # MeanWindSpeed, NWindObs, MaxSustWindSpeed, MaxWindGust\n",
      "            if row[14] == \"0\":\n",
      "                # if there are 0 observations, then set a bunch of nulls\n",
      "                outrow.append(\"NULL\")\n",
      "                outrow.append(\"0\")\n",
      "                outrow.append(\"NULL\")\n",
      "                outrow.append(\"NULL\")\n",
      "            else:\n",
      "                for i in range(13, 17, 1):\n",
      "                    if row[i] == \"999.9\": outrow.append(\"NULL\")\n",
      "                    else: outrow.append(row[i])\n",
      "\n",
      "            # Temp fields may or may not have a \"*\" appended after the number, so we\n",
      "            #        handle these by first checking what the last character is:\n",
      "            # \"MaxTemp\", \"MaxTempSource\", \"MinTemp\", \"MinTempSource\"\n",
      "            for i in range(17, 19, 1):\n",
      "                if (row[i])[-1] == \"*\":\n",
      "                    # then the flag is present, indicating the source was derived\n",
      "                    #        indirectly from hourly data\n",
      "                    outrow.append((row[i])[:-1])\n",
      "                    outrow.append(\"hourly\")\n",
      "                else:\n",
      "                    # if it's not present then this was an explicit max/min reading\n",
      "                    outrow.append(row[i])\n",
      "                    outrow.append(\"explicit\")\n",
      "\n",
      "            # Precipitation has its own extra special flag source and NULL placeholder\n",
      "            # PrecipAmount, NPrecipReportHours, PrecipFlag\n",
      "            if row[19] == \"99.99\":\n",
      "                # then it's null, so:\n",
      "                outrow.append(\"NULL\")\n",
      "                outrow.append(\"NULL\")\n",
      "                outrow.append(\"NULL\")\n",
      "            else:\n",
      "                outrow.append((row[19])[:-1])\n",
      "                # translations of the flag, as per\n",
      "                #        ftp://ftp.ncdc.noaa.gov/pub/data/gsod/readme.txt\n",
      "                if (row[19])[-1] == \"A\": outrow.append(\"6\")\n",
      "                elif (row[19])[-1] == \"B\": outrow.append(\"12\")\n",
      "                elif (row[19])[-1] == \"C\": outrow.append(\"18\")\n",
      "                elif (row[19])[-1] == \"D\": outrow.append(\"24\")\n",
      "                elif (row[19])[-1] == \"E\": outrow.append(\"12\")\n",
      "                elif (row[19])[-1] == \"F\": outrow.append(\"24\")\n",
      "                elif (row[19])[-1] == \"G\": outrow.append(\"24\")\n",
      "                elif (row[19])[-1] == \"H\": outrow.append(\"0\")\n",
      "                elif (row[19])[-1] == \"I\": outrow.append(\"0\")\n",
      "                else: outrow.append(\"ERR\")\n",
      "                outrow.append((row[19])[-1])\n",
      "\n",
      "            # SnowDepth is relatively straightforward\n",
      "            if row[20] == \"999.9\":\n",
      "                outrow.append(\"NULL\")\n",
      "            else:\n",
      "                outrow.append(row[20])\n",
      "\n",
      "            # Fog, Rain, Snow, Hail, Thunder, Tornado\n",
      "            # these are stored as one six-bit binary string, so we unpack it here\n",
      "            for i in range(0, 6, 1):\n",
      "                outrow.append((row[21])[i])\n",
      "\n",
      "            # And we're done!  Now write the row to the output file\n",
      "            writer.writerow(outrow)\n",
      "    f_in.close()\n",
      "    f_out.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now go through every airport and download its weather data to a separate csv\n",
      "years = [2011]\n",
      "URLroot = \"ftp://ftp.ncdc.noaa.gov/pub/data/gsod/\" # base URL for all files\n",
      "filesuffix = \".op.gz\" # suffix for all the raw files\n",
      "savepath_root = 'maindata/gsod/'\n",
      "    \n",
      "for year in years:\n",
      "    for station in stations:\n",
      "        for i in range(len(stations[station]['wban'])):\n",
      "            # construct the appropriate url\n",
      "            WBAN = stations[station]['wban'][i]\n",
      "            USAF = stations[station]['usaf'][i]\n",
      "            stationcode = str(USAF) + '-' + str(WBAN)\n",
      "            fullURL = (URLroot + str(year) + '/' + stationcode + '-' +\n",
      "                str(year) + filesuffix)\n",
      "\n",
      "            # try running the scraper. If the first USAF-WBAN code combo does not exist, \n",
      "            # try any additional ones\n",
      "            if downloadfiles(fullURL, filesuffix, station, parsegsod, savepath_root+str(year)+'/') == 1:\n",
      "                break\n",
      "            # if none of the codes have worked\n",
      "            elif i == len(stations[station]['wban']) - 1:\n",
      "                print 'Failed to find any data for ' + station + ' in year ' + str(year)\n",
      "                print\n",
      "            else:\n",
      "                continue\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Successfully downloaded 2011 for station MCO\n",
        "Successfully downloaded 2011 for station JFK"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station BOS"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station IAH"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station PHL"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station MIA"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station ATL"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station LGA"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station MSP"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station SEA"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station SFO"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station PHX"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station LAX"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "DOWNLOADING ERROR: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [Errno ftp error] [Errno ftp error] 550 724670-03017-2011.op.gz: No such file or directory\n",
        "Successfully downloaded 2011 for station DEN"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station CLT"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station EWR"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station ORD"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station DTW"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station DFW"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station LAS"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Scraping historical <u>daily</u> weather data from NOAA\n",
      "\n",
      "Having obtained records for every day of the year, we then wanted to scrape two more datasets: one for hourly temperature, another for hourly precipitation. Hourly data was obtained from the NOAA Integrated Surface Data-Lite (ISD-Lite) Dataset.\n",
      "\n",
      "To do this, we have rewritten the csv parser file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This function goes through each downloaded file line by line, and translates\n",
      "#        it from NOAA's idiosyncratic format to CSV with all the fields separated\n",
      "#        out rationally.\n",
      "def parseisd(filename, station, savepath):\n",
      "    # open the input file\n",
      "    f_in = gzip.open(str(filename))\n",
      "    reader = csv.reader(f_in, delimiter=' ', quoting=csv.QUOTE_NONE, skipinitialspace=True)\n",
      "\n",
      "    # open the output file and write the header row\n",
      "    f_out = open(savepath+station+'.csv','w')\n",
      "    writer = csv.writer(f_out, dialect=csv.excel)\n",
      "    writer.writerow([\"Station\", \"Datetime\",\n",
      "        \"AirTemp(C)\", \"DewpointTemp\", \"SeaLevelPressure(100Pa)\", \"WindSpeed(m/s)\",\"Precip(mm)\"])\n",
      "\n",
      "    # iterate through the rows of the file, adding \n",
      "    for row in reader:\n",
      "        if row[0] != 'STN---':\n",
      "            # If it's the header row, just skip; otherwise process\n",
      "            outrow = [station] # the station name is not in the input file\n",
      "\n",
      "            # col 0-3 are datetime fields; turn them into a single datetime string\n",
      "            dt = row[0] + '-' + row[1] + '-' + row[2] + ' '+ row[3] + ':00:00'\n",
      "            outrow.append(dt)\n",
      "            \n",
      "            # specify the fields we're actually interested in keeping\n",
      "            datarows = (4, 5, 6, 8, 10)\n",
      "            for r in datarows:\n",
      "                # NOAA uses '-9999' for missing data, we will uses nan \n",
      "                if row[r] == '-9999':\n",
      "                    outrow.append('nan')\n",
      "                else:\n",
      "                    outrow.append(row[r])\n",
      "\n",
      "            # And we're done!  Now write the row to the output file\n",
      "            writer.writerow(outrow)\n",
      "    f_in.close()\n",
      "    f_out.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now we download all the hourly data\n",
      "years = [2011]\n",
      "URLroot = 'ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-lite/' # base URL for all files\n",
      "filesuffix = '.gz' # suffix for all the raw files\n",
      "savepath_root = 'maindata/isd-lite/'\n",
      "    \n",
      "for year in years:\n",
      "    for station in stations:\n",
      "        for i in range(len(stations[station]['wban'])):\n",
      "            # construct the appropriate url\n",
      "            WBAN = stations[station]['wban'][i]\n",
      "            USAF = stations[station]['usaf'][i]\n",
      "            stationcode = str(USAF) + '-' + str(WBAN)\n",
      "            fullURL = (URLroot + str(year) + '/' + stationcode + '-' +\n",
      "                str(year) + filesuffix)\n",
      "\n",
      "            # try running the scraper. If the first USAF-WBAN code combo does not exist, \n",
      "            # try any additional ones\n",
      "            if downloadfiles(fullURL, filesuffix, station, parseisd, savepath_root+str(year)+'/') == 1:\n",
      "                break\n",
      "            # if none of the codes have worked\n",
      "            elif i == len(stations[station]['wban']) - 1:\n",
      "                print 'Failed to find any data for ' + station + ' in year ' + str(year)\n",
      "                print\n",
      "            else:\n",
      "                continue"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Successfully downloaded 2011 for station MCO\n",
        "Successfully downloaded 2011 for station JFK"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station BOS"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station IAH"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station PHL"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station MIA"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station ATL"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station LGA"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station MSP"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station SEA"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station SFO"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station PHX"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station LAX"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "DOWNLOADING ERROR: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " [Errno ftp error] [Errno ftp error] 550 724670-03017-2011.gz: No such file or directory\n",
        "Successfully downloaded 2011 for station DEN"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station CLT"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station EWR"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station ORD"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station DTW"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station DFW"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Successfully downloaded 2011 for station LAS"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Putting everything into pandas\n",
      "\n",
      "With the data in hand, our next step was to combine everything into a pandas dataframe. We wanted the relevant columns from each dataset combined into a master database. We guessed that some relevant fields would include temperature, visibility, dew point, wind speed, amount of precipitation, type of precipitation, and occurence of extreme weather events, requiring a bit of data from each dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# master dataframe\n",
      "df = pd.DataFrame()\n",
      "for year in years:\n",
      "    for station in stations:\n",
      "        filenamegsod = 'maindata/gsod/' + str(year) + '/' + station + '.csv'\n",
      "        filenameisd = 'maindata/isd-lite/' + str(year) + '/' + station + '.csv'\n",
      "        # read in the gsod and isd datasets\n",
      "        gsod = pd.read_csv(filenamegsod, parse_dates={'Date': [1,2,3]})\n",
      "        gsod.set_index('Date', inplace=True)\n",
      "        isd = pd.read_csv(filenameisd, parse_dates=True)\n",
      "        \n",
      "        # create a new dataframe that duplicates gsod rows 24 times each\n",
      "        # so that there is a row for each hour, preserving only columns we're interested in\n",
      "        nRows = 24*len(gsod)\n",
      "        gsodhours = np.zeros((nRows,7))\n",
      "        counter = 0\n",
      "        for _, row in gsod.iterrows():\n",
      "            lo = counter\n",
      "            hi = counter + 24\n",
      "            gsodhours[lo:hi, 0] = row['Visibility']\n",
      "            gsodhours[lo:hi, 1] = row['Fog']\n",
      "            gsodhours[lo:hi, 2] = row['Rain']\n",
      "            gsodhours[lo:hi, 3] = row['Snow']\n",
      "            gsodhours[lo:hi, 4] = row['Hail']\n",
      "            gsodhours[lo:hi, 5] = row['Thunder']\n",
      "            gsodhours[lo:hi, 6] = row['Tornado']\n",
      "            counter += 24\n",
      "        gsodnew = pd.DataFrame(gsodhours, columns=['Visibility', 'Fog',\n",
      "                                                   'Rain', 'Snow', 'Hail',\n",
      "                                                   'Thunder', 'Tornado'])\n",
      "        # concatenate the desired gsod columns to the isd dataframe\n",
      "        stationdf = pd.concat([isd, gsodnew], axis=1, join='outer')\n",
      "    \n",
      "        # now concatenate these new rows to the master data frame with data from all airports\n",
      "        df = pd.concat([df, stationdf], axis=0, join='outer')\n",
      "    \n",
      "    # rename some columns and make some units adjustments (some of the units of the original data are off by a decimal point)\n",
      "    df = df.rename(columns={'AirTemp(C)':'Temp', 'DewpointTemp':'Dewpoint', \n",
      "                       'Precip(mm)':'Precip', 'SeaLevelPressure(100Pa)':'Pressure',\n",
      "                       'WindSpeed(m/s)':'Windspeed'})\n",
      "    df.Temp *= 0.1\n",
      "    df.Dewpoint *= 0.1\n",
      "    df.Precip *= -1\n",
      "    df.Pressure *= 0.01\n",
      "    df.Windspeed *= 0.1\n",
      "    \n",
      "    # write the master dataframe to a csv\n",
      "    df.to_csv('maindata/weatherdata_complete_' + str(year) + '.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The Result\n",
      "\n",
      "We now have a weather dataframe with weather data for the top 20 busiest US airports in 2012, with the following columns:\n",
      "\n",
      "<b>Station:</b> The airport weather station where the data was collected<br/>\n",
      "<b>Datetime:</b> A python datetime object, with one entry for every hour of every day at every station<br/>\n",
      "<b>Temp:</b> Air temperature in \u02daC, measured hourly<br/>\n",
      "<b>Dewpoint:</b> Dewpoint temperature in \u02daC, measured hourly<br/>\n",
      "<b>Windspeed:</b> Wind speed in m/s, measured hourly<br/>\n",
      "<b>Pressure:</b> Sea Level Pressure in kPa, measured hourly<br/>\n",
      "<b>Visibility:</b> Visibility in miles (appears to max out at 10), measured daily<br/>\n",
      "<b>Fog:</b>Boolean for presence of fog, measured daily<br/>\n",
      "<b>Rain:</b>Boolean for presence of rain, measured daily<br/>\n",
      "<b>Snow:</b>Boolean for presence of snow, measured daily<br/>\n",
      "<b>Hail:</b>Boolean for presence of hail, measured daily<br/>\n",
      "<b>Thunder:</b>Boolean for presence of thunderstorms, measured daily<br/>\n",
      "<b>Tornado:</b>Boolean for presence of tornadoes, measured daily<br/>\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}